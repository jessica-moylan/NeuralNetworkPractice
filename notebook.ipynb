{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessica/miniconda3/envs/torch-test/lib/python3.12/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2277.735107421875\n",
      "199 1580.0631103515625\n",
      "299 1097.9927978515625\n",
      "399 764.551025390625\n",
      "499 533.6798095703125\n",
      "599 373.66845703125\n",
      "699 262.6610107421875\n",
      "799 185.578125\n",
      "899 132.00323486328125\n",
      "999 94.73424530029297\n",
      "1099 68.7862319946289\n",
      "1199 50.70525360107422\n",
      "1299 38.09626388549805\n",
      "1399 29.2965087890625\n",
      "1499 23.150659561157227\n",
      "1599 18.855321884155273\n",
      "1699 15.851286888122559\n",
      "1799 13.74896240234375\n",
      "1899 12.276814460754395\n",
      "1999 11.245322227478027\n",
      "Result: y = 0.04898932948708534 + 0.8402983546257019 x + -0.008451475761830807 x^2 + -0.09099159389734268 x^3\n"
     ]
    }
   ],
   "source": [
    "# Code from \n",
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 232 entries, 0 to 231\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Date    232 non-null    object\n",
      " 1   Open    232 non-null    object\n",
      " 2   High    232 non-null    object\n",
      " 3   Low     232 non-null    object\n",
      " 4   Close   232 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 9.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"stock-data.csv\")\n",
    "df.info()\n",
    "df.drop(\"Date\", inplace=True, axis = 1)\n",
    "df = df.replace(\",\",\"\", regex = True)\n",
    "df = df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 232 entries, 0 to 231\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Open    232 non-null    float64\n",
      " 1   High    232 non-null    float64\n",
      " 2   Low     232 non-null    float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 5.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "features = df[[\"Open\", \"High\", \"Low\"]]\n",
    "print(features.info())\n",
    "features = features.values\n",
    "target = df[[\"Close\"]].values\n",
    "# features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = features.mean(axis = 0)\n",
    "# std = features.std(axis=0)\n",
    "# features = (features - mean)/std\n",
    "\n",
    "features = torch.round(torch.tensor(features, dtype = torch.float64)*100/100)\n",
    "\n",
    "#this is beacuse we want a 1D array for our answerers\n",
    "target = torch.round(torch.tensor(target, dtype = torch.float64).view(-1,1)*100)/100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class stockPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(stockPredictor, self).__init__()\n",
    "        #this data has 4 inputs than we want to go to 64 i dont not understand why we chose these numbers\n",
    "        self.fc1 = nn.Linear(3,16)\n",
    "        self.fc2 = nn.Linear(16,63)\n",
    "        self.fc3 = nn.Linear(63,32)\n",
    "        self.fc5 = nn.Linear(32,1)\n",
    "        self.double()\n",
    "    \n",
    "    #goes through each layer of the neural netwrok and returns an answer depending\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = stockPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4536.3400],\n",
      "        [5234.1800],\n",
      "        [4754.6300],\n",
      "        [5307.0100],\n",
      "        [5070.5500],\n",
      "        [5071.6300],\n",
      "        [5204.3400],\n",
      "        [5178.5100],\n",
      "        [4845.6500],\n",
      "        [5243.7700],\n",
      "        [4719.1900],\n",
      "        [4585.5900],\n",
      "        [4890.9700],\n",
      "        [4411.5900],\n",
      "        [4643.7000],\n",
      "        [4193.8000],\n",
      "        [4508.2400],\n",
      "        [5199.0600],\n",
      "        [5308.1500],\n",
      "        [4522.7900],\n",
      "        [5104.7600],\n",
      "        [4997.9100],\n",
      "        [4186.7700],\n",
      "        [4373.6300],\n",
      "        [5149.4200],\n",
      "        [4247.6800],\n",
      "        [4465.4800],\n",
      "        [4376.3100],\n",
      "        [5137.0800],\n",
      "        [5203.5800],\n",
      "        [4274.5100],\n",
      "        [5087.0300],\n",
      "        [4468.8300],\n",
      "        [4505.4200],\n",
      "        [5096.2700]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size = 0.15, random_state = 41)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessica/miniconda3/envs/torch-test/lib/python3.12/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500], Loss: 19647658.400308494\n",
      "Epoch [20/500], Loss: 16390940.253635852\n",
      "Epoch [30/500], Loss: 11285505.318657793\n",
      "Epoch [40/500], Loss: 4492711.204695241\n",
      "Epoch [50/500], Loss: 57291.40469835453\n",
      "Epoch [60/500], Loss: 605170.870265514\n",
      "Epoch [70/500], Loss: 3712.9287641215537\n",
      "Epoch [80/500], Loss: 76018.54899937924\n",
      "Epoch [90/500], Loss: 2679.2273216165695\n",
      "Epoch [100/500], Loss: 9931.838577938415\n",
      "Epoch [110/500], Loss: 425.59076679619943\n",
      "Epoch [120/500], Loss: 1741.383994205673\n",
      "Epoch [130/500], Loss: 409.35777031793634\n",
      "Epoch [140/500], Loss: 516.5445928012398\n",
      "Epoch [150/500], Loss: 426.42722746666885\n",
      "Epoch [160/500], Loss: 406.92131925554327\n",
      "Epoch [170/500], Loss: 400.53377954954686\n",
      "Epoch [180/500], Loss: 397.2025483736417\n",
      "Epoch [190/500], Loss: 398.7136129780437\n",
      "Epoch [200/500], Loss: 396.6615634245759\n",
      "Epoch [210/500], Loss: 396.8745437439265\n",
      "Epoch [220/500], Loss: 397.182392982441\n",
      "Epoch [230/500], Loss: 396.8359422396649\n",
      "Epoch [240/500], Loss: 396.88022774250925\n",
      "Epoch [250/500], Loss: 396.926777233687\n",
      "Epoch [260/500], Loss: 396.8511222988724\n",
      "Epoch [270/500], Loss: 396.85447155952437\n",
      "Epoch [280/500], Loss: 396.849719334957\n",
      "Epoch [290/500], Loss: 396.8253976416481\n",
      "Epoch [300/500], Loss: 396.8187270355597\n",
      "Epoch [310/500], Loss: 396.80711015465977\n",
      "Epoch [320/500], Loss: 396.79417178758627\n",
      "Epoch [330/500], Loss: 396.7848183264592\n",
      "Epoch [340/500], Loss: 396.7739404780551\n",
      "Epoch [350/500], Loss: 396.7638173980199\n",
      "Epoch [360/500], Loss: 396.7542824862025\n",
      "Epoch [370/500], Loss: 396.74463419221155\n",
      "Epoch [380/500], Loss: 396.73546421770095\n",
      "Epoch [390/500], Loss: 396.7264524455477\n",
      "Epoch [400/500], Loss: 396.71763212713074\n",
      "Epoch [410/500], Loss: 396.7090407588709\n",
      "Epoch [420/500], Loss: 396.70059537550407\n",
      "Epoch [430/500], Loss: 396.69232501162185\n",
      "Epoch [440/500], Loss: 396.6841991906631\n",
      "Epoch [450/500], Loss: 396.6762080647372\n",
      "Epoch [460/500], Loss: 396.6683448991203\n",
      "Epoch [470/500], Loss: 396.66059523847326\n",
      "Epoch [480/500], Loss: 396.6529531481321\n",
      "Epoch [490/500], Loss: 396.64540874452354\n",
      "Epoch [500/500], Loss: 396.63795486823955\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 0.001)\n",
    "\n",
    "epochs = 500\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    y_pred = model.forward(x_train)\n",
    "\n",
    "    outputs = model(features)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    losses.append(loss.detach().numpy())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch +1) %10 ==0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_eval = model.forward(x_test)\n",
    "    loss = criterion(y_eval,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 4546.92 Expected: 4536.34 Difference: -10.579999999999927\n",
      "Predicted: 5238.92 Expected: 5234.18 Difference: -4.739999999999782\n",
      "Predicted: 4755.28 Expected: 4754.63 Difference: -0.6499999999996362\n",
      "Predicted: 5309.85 Expected: 5307.01 Difference: -2.8400000000001455\n",
      "Predicted: 5045.39 Expected: 5070.55 Difference: 25.159999999999854\n",
      "Predicted: 5074.47 Expected: 5071.63 Difference: -2.8400000000001455\n",
      "Predicted: 5181.1 Expected: 5204.34 Difference: 23.23999999999978\n",
      "Predicted: 5151.36 Expected: 5178.51 Difference: 27.150000000000546\n",
      "Predicted: 4885.43 Expected: 4845.65 Difference: -39.780000000000655\n",
      "Predicted: 5250.77 Expected: 5243.77 Difference: -7.0\n",
      "Predicted: 4714.96 Expected: 4719.19 Difference: 4.229999999999563\n",
      "Predicted: 4575.21 Expected: 4585.59 Difference: 10.38000000000011\n",
      "Predicted: 4892.46 Expected: 4890.97 Difference: -1.4899999999997817\n",
      "Predicted: 4411.15 Expected: 4411.59 Difference: 0.4400000000005093\n",
      "Predicted: 4623.98 Expected: 4643.7 Difference: 19.720000000000255\n",
      "Predicted: 4174.43 Expected: 4193.8 Difference: 19.36999999999989\n",
      "Predicted: 4499.15 Expected: 4508.24 Difference: 9.090000000000146\n",
      "Predicted: 5176.82 Expected: 5199.06 Difference: 22.24000000000069\n",
      "Predicted: 5280.36 Expected: 5308.15 Difference: 27.789999999999964\n",
      "Predicted: 4515.99 Expected: 4522.79 Difference: 6.800000000000182\n",
      "Predicted: 5109.85 Expected: 5104.76 Difference: -5.0900000000001455\n",
      "Predicted: 4993.52 Expected: 4997.91 Difference: 4.389999999999418\n",
      "Predicted: 4216.46 Expected: 4186.77 Difference: -29.6899999999996\n",
      "Predicted: 4356.61 Expected: 4373.63 Difference: 17.020000000000437\n",
      "Predicted: 5158.94 Expected: 5149.42 Difference: -9.519999999999527\n",
      "Predicted: 4238.94 Expected: 4247.68 Difference: 8.740000000000691\n",
      "Predicted: 4475.25 Expected: 4465.48 Difference: -9.770000000000437\n",
      "Predicted: 4432.47 Expected: 4376.31 Difference: -56.159999999999854\n",
      "Predicted: 5111.95 Expected: 5137.08 Difference: 25.13000000000011\n",
      "Predicted: 5222.65 Expected: 5203.58 Difference: -19.06999999999971\n",
      "Predicted: 4272.87 Expected: 4274.51 Difference: 1.6400000000003274\n",
      "Predicted: 5058.69 Expected: 5087.03 Difference: 28.340000000000146\n",
      "Predicted: 4492.83 Expected: 4468.83 Difference: -24.0\n",
      "Predicted: 4514.66 Expected: 4505.42 Difference: -9.239999999999782\n",
      "Predicted: 5084.84 Expected: 5096.27 Difference: 11.430000000000291\n",
      "Correct: 0\n",
      "19.349463851666304\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "resid = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(x_test):\n",
    "        y_val = model.forward(data)\n",
    "        resid.append(round(y_test[i].item(),2) - round(y_val.item(),2))\n",
    "        print(f\"Predicted: {round(y_val.item(),2)} Expected: {round(y_test[i].item(),2)} Difference: {round(y_test[i].item(),2) - round(y_val.item(),2)}\")\n",
    "        if round(y_val.item(),2) == round(y_test[i].item(),2):\n",
    "            num_correct += 1\n",
    "print(f\"Correct: {num_correct}\")\n",
    "print(np.std(resid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 4763.35 Expected: 4780.94 Difference: 17.589999999999236\n",
      "Predicted: 4343.5 Expected: 4314.6 Difference: -28.899999999999636\n",
      "Predicted: 4473.2 Expected: 4472.16 Difference: -1.0399999999999636\n",
      "Predicted: 4561.18 Expected: 4569.78 Difference: 8.599999999999454\n",
      "Predicted: 5310.54 Expected: 5308.13 Difference: -2.4099999999998545\n",
      "Predicted: 4441.34 Expected: 4451.14 Difference: 9.800000000000182\n",
      "Predicted: 4774.96 Expected: 4769.83 Difference: -5.130000000000109\n",
      "Predicted: 4752.45 Expected: 4768.37 Difference: 15.920000000000073\n",
      "Predicted: 4518.86 Expected: 4507.66 Difference: -11.199999999999818\n",
      "Predicted: 5085.67 Expected: 5035.69 Difference: -49.98000000000047\n",
      "Predicted: 4511.66 Expected: 4478.03 Difference: -33.63000000000011\n",
      "Predicted: 4383.93 Expected: 4399.77 Difference: 15.840000000000146\n",
      "Predicted: 4555.42 Expected: 4556.62 Difference: 1.199999999999818\n",
      "Predicted: 5121.6 Expected: 5127.79 Difference: 6.1899999999996\n",
      "Predicted: 4356.12 Expected: 4358.24 Difference: 2.119999999999891\n",
      "Predicted: 4360.61 Expected: 4373.2 Difference: 12.590000000000146\n",
      "Predicted: 4769.56 Expected: 4783.45 Difference: 13.889999999999418\n",
      "Predicted: 4251.66 Expected: 4258.19 Difference: 6.529999999999745\n",
      "Predicted: 4427.22 Expected: 4433.31 Difference: 6.0900000000001455\n",
      "Predicted: 5209.24 Expected: 5202.39 Difference: -6.849999999999454\n",
      "Predicted: 4741.07 Expected: 4742.83 Difference: 1.7600000000002183\n",
      "Predicted: 4554.54 Expected: 4567.8 Difference: 13.260000000000218\n",
      "Predicted: 4575.42 Expected: 4549.34 Difference: -26.079999999999927\n",
      "Predicted: 4537.1 Expected: 4513.39 Difference: -23.710000000000036\n",
      "Predicted: 5153.58 Expected: 5123.41 Difference: -30.170000000000073\n",
      "Predicted: 4775.38 Expected: 4781.58 Difference: 6.199999999999818\n",
      "Predicted: 5121.71 Expected: 5117.09 Difference: -4.619999999999891\n",
      "Predicted: 4573.55 Expected: 4582.23 Difference: 8.679999999999382\n",
      "Predicted: 5046.04 Expected: 5064.2 Difference: 18.159999999999854\n",
      "Predicted: 4220.48 Expected: 4217.04 Difference: -3.4399999999996\n",
      "Predicted: 4535.46 Expected: 4538.19 Difference: 2.7299999999995634\n",
      "Predicted: 5159.08 Expected: 5123.69 Difference: -35.39000000000033\n",
      "Predicted: 4351.71 Expected: 4327.78 Difference: -23.93000000000029\n",
      "Predicted: 5097.92 Expected: 5088.8 Difference: -9.11999999999989\n",
      "Predicted: 5304.76 Expected: 5306.04 Difference: 1.2799999999997453\n",
      "Predicted: 4430.5 Expected: 4404.33 Difference: -26.170000000000073\n",
      "Predicted: 4482.39 Expected: 4450.32 Difference: -32.07000000000062\n",
      "Predicted: 5067.1 Expected: 5069.76 Difference: 2.6599999999998545\n",
      "Predicted: 4884.94 Expected: 4894.16 Difference: 9.220000000000255\n",
      "Predicted: 4549.04 Expected: 4554.64 Difference: 5.600000000000364\n",
      "Predicted: 5202.76 Expected: 5209.91 Difference: 7.149999999999636\n",
      "Predicted: 4348.23 Expected: 4358.34 Difference: 10.110000000000582\n",
      "Predicted: 4501.37 Expected: 4518.44 Difference: 17.06999999999971\n",
      "Predicted: 4734.08 Expected: 4740.56 Difference: 6.480000000000473\n",
      "Predicted: 5188.49 Expected: 5187.7 Difference: -0.7899999999999636\n",
      "Predicted: 4584.0 Expected: 4588.96 Difference: 4.960000000000036\n",
      "Predicted: 5279.02 Expected: 5291.34 Difference: 12.319999999999709\n",
      "Predicted: 5162.31 Expected: 5160.64 Difference: -1.6700000000000728\n",
      "Predicted: 4563.58 Expected: 4566.75 Difference: 3.1700000000000728\n",
      "Predicted: 5279.59 Expected: 5283.4 Difference: 3.8099999999994907\n",
      "Predicted: 4500.22 Expected: 4501.89 Difference: 1.6700000000000728\n",
      "Predicted: 4719.03 Expected: 4719.55 Difference: 0.5200000000004366\n",
      "Predicted: 5062.38 Expected: 5051.41 Difference: -10.970000000000255\n",
      "Predicted: 4668.34 Expected: 4707.09 Difference: 38.75\n",
      "Predicted: 4402.3 Expected: 4370.36 Difference: -31.94000000000051\n",
      "Predicted: 4355.21 Expected: 4369.71 Difference: 14.5\n",
      "Predicted: 4947.65 Expected: 4954.23 Difference: 6.579999999999927\n",
      "Predicted: 4566.33 Expected: 4565.72 Difference: -0.6099999999996726\n",
      "Predicted: 4257.74 Expected: 4229.45 Difference: -28.289999999999964\n",
      "Predicted: 4493.55 Expected: 4505.1 Difference: 11.550000000000182\n",
      "Predicted: 4164.37 Expected: 4137.23 Difference: -27.140000000000327\n",
      "Predicted: 5024.02 Expected: 5005.57 Difference: -18.450000000000728\n",
      "Predicted: 5219.54 Expected: 5147.21 Difference: -72.32999999999993\n",
      "Predicted: 4263.44 Expected: 4308.5 Difference: 45.0600000000004\n",
      "Predicted: 4698.76 Expected: 4697.24 Difference: -1.5200000000004366\n",
      "Predicted: 4312.61 Expected: 4278.0 Difference: -34.60999999999967\n",
      "Predicted: 4399.13 Expected: 4409.53 Difference: 10.399999999999636\n",
      "Predicted: 5206.1 Expected: 5211.49 Difference: 5.389999999999418\n",
      "Predicted: 4994.53 Expected: 4967.23 Difference: -27.300000000000182\n",
      "Predicted: 5251.43 Expected: 5241.53 Difference: -9.900000000000546\n",
      "Predicted: 4580.57 Expected: 4537.41 Difference: -43.159999999999854\n",
      "Predicted: 4489.17 Expected: 4499.38 Difference: 10.210000000000036\n",
      "Predicted: 4313.75 Expected: 4288.05 Difference: -25.699999999999818\n",
      "Predicted: 5195.11 Expected: 5214.08 Difference: 18.970000000000255\n",
      "Predicted: 5315.68 Expected: 5267.84 Difference: -47.840000000000146\n",
      "Predicted: 5142.58 Expected: 5157.36 Difference: 14.779999999999745\n",
      "Predicted: 4718.32 Expected: 4704.81 Difference: -13.509999999999309\n",
      "Predicted: 5198.63 Expected: 5205.81 Difference: 7.180000000000291\n",
      "Predicted: 4423.37 Expected: 4439.26 Difference: 15.890000000000327\n",
      "Predicted: 4259.59 Expected: 4224.16 Difference: -35.43000000000029\n",
      "Predicted: 5095.86 Expected: 5078.65 Difference: -17.210000000000036\n",
      "Predicted: 5030.98 Expected: 5021.84 Difference: -9.139999999999418\n",
      "Predicted: 5091.78 Expected: 5099.96 Difference: 8.180000000000291\n",
      "Predicted: 5031.72 Expected: 5011.12 Difference: -20.600000000000364\n",
      "Predicted: 5221.12 Expected: 5218.19 Difference: -2.930000000000291\n",
      "Predicted: 4787.81 Expected: 4783.83 Difference: -3.980000000000473\n",
      "Predicted: 5241.15 Expected: 5277.51 Difference: 36.36000000000058\n",
      "Predicted: 4551.93 Expected: 4554.89 Difference: 2.9600000000000364\n",
      "Predicted: 5086.88 Expected: 5069.53 Difference: -17.350000000000364\n",
      "Predicted: 4241.96 Expected: 4263.75 Difference: 21.789999999999964\n",
      "Predicted: 5225.21 Expected: 5222.68 Difference: -2.5299999999997453\n",
      "Predicted: 5227.04 Expected: 5221.42 Difference: -5.619999999999891\n",
      "Predicted: 5159.66 Expected: 5150.48 Difference: -9.180000000000291\n",
      "Predicted: 5307.0 Expected: 5321.41 Difference: 14.409999999999854\n",
      "Predicted: 4603.76 Expected: 4622.44 Difference: 18.67999999999938\n",
      "Predicted: 5290.7 Expected: 5304.72 Difference: 14.020000000000437\n",
      "Predicted: 4377.33 Expected: 4347.35 Difference: -29.979999999999563\n",
      "Predicted: 4318.4 Expected: 4337.44 Difference: 19.039999999999964\n",
      "Predicted: 4282.98 Expected: 4288.39 Difference: 5.410000000000764\n",
      "Predicted: 4727.85 Expected: 4746.75 Difference: 18.899999999999636\n",
      "Predicted: 4934.93 Expected: 4958.61 Difference: 23.67999999999938\n",
      "Predicted: 4732.98 Expected: 4739.21 Difference: 6.230000000000473\n",
      "Predicted: 4778.28 Expected: 4780.24 Difference: 1.9600000000000364\n",
      "Predicted: 5323.34 Expected: 5354.03 Difference: 30.6899999999996\n",
      "Predicted: 5109.76 Expected: 5117.94 Difference: 8.179999999999382\n",
      "Predicted: 4378.84 Expected: 4382.78 Difference: 3.9399999999996\n",
      "Predicted: 5048.62 Expected: 5018.39 Difference: -30.229999999999563\n",
      "Predicted: 5053.45 Expected: 5022.21 Difference: -31.23999999999978\n",
      "Predicted: 4369.5 Expected: 4378.38 Difference: 8.88000000000011\n",
      "Predicted: 4525.2 Expected: 4515.77 Difference: -9.429999999999382\n",
      "Predicted: 4767.58 Expected: 4765.98 Difference: -1.6000000000003638\n",
      "Predicted: 4724.9 Expected: 4763.54 Difference: 38.64000000000033\n",
      "Predicted: 4457.52 Expected: 4464.05 Difference: 6.529999999999745\n",
      "Predicted: 4306.71 Expected: 4335.66 Difference: 28.949999999999818\n",
      "Predicted: 5230.1 Expected: 5248.49 Difference: 18.389999999999418\n",
      "Predicted: 4980.99 Expected: 4995.06 Difference: 14.070000000000618\n",
      "Predicted: 4979.38 Expected: 5000.62 Difference: 21.23999999999978\n",
      "Predicted: 4507.24 Expected: 4502.88 Difference: -4.359999999999673\n",
      "Predicted: 4572.01 Expected: 4594.63 Difference: 22.61999999999989\n",
      "Predicted: 5070.99 Expected: 5078.18 Difference: 7.190000000000509\n",
      "Predicted: 5252.71 Expected: 5254.35 Difference: 1.6400000000003274\n",
      "Predicted: 4480.12 Expected: 4487.46 Difference: 7.3400000000001455\n",
      "Predicted: 4527.49 Expected: 4547.38 Difference: 19.890000000000327\n",
      "Predicted: 4489.27 Expected: 4467.71 Difference: -21.5600000000004\n",
      "Predicted: 4476.97 Expected: 4495.7 Difference: 18.729999999999563\n",
      "Predicted: 4286.78 Expected: 4317.78 Difference: 31.0\n",
      "Predicted: 4339.3 Expected: 4320.06 Difference: -19.23999999999978\n",
      "Predicted: 4855.63 Expected: 4864.6 Difference: 8.970000000000255\n",
      "Predicted: 5127.46 Expected: 5061.82 Difference: -65.64000000000033\n",
      "Predicted: 4389.56 Expected: 4405.71 Difference: 16.149999999999636\n",
      "Predicted: 5229.96 Expected: 5246.68 Difference: 16.720000000000255\n",
      "Predicted: 5310.48 Expected: 5297.1 Difference: -13.3799999999992\n",
      "Predicted: 4360.91 Expected: 4330.0 Difference: -30.909999999999854\n",
      "Predicted: 4964.6 Expected: 4981.8 Difference: 17.199999999999818\n",
      "Predicted: 4954.25 Expected: 4953.17 Difference: -1.0799999999999272\n",
      "Predicted: 4457.11 Expected: 4497.63 Difference: 40.52000000000044\n",
      "Predicted: 5168.1 Expected: 5165.31 Difference: -2.7899999999999636\n",
      "Predicted: 4746.24 Expected: 4756.5 Difference: 10.260000000000218\n",
      "Predicted: 4980.03 Expected: 4975.51 Difference: -4.519999999999527\n",
      "Predicted: 4509.97 Expected: 4514.02 Difference: 4.050000000000182\n",
      "Predicted: 4767.53 Expected: 4774.75 Difference: 7.220000000000255\n",
      "Predicted: 4809.48 Expected: 4839.81 Difference: 30.330000000000837\n",
      "Predicted: 5155.62 Expected: 5180.74 Difference: 25.11999999999989\n",
      "Predicted: 5000.39 Expected: 5010.6 Difference: 10.210000000000036\n",
      "Predicted: 4855.05 Expected: 4850.43 Difference: -4.619999999999891\n",
      "Predicted: 4549.73 Expected: 4534.87 Difference: -14.859999999999673\n",
      "Predicted: 4298.59 Expected: 4273.53 Difference: -25.0600000000004\n",
      "Predicted: 4412.86 Expected: 4436.01 Difference: 23.150000000000546\n",
      "Predicted: 4150.8 Expected: 4166.82 Difference: 16.019999999999527\n",
      "Predicted: 4285.16 Expected: 4299.7 Difference: 14.539999999999964\n",
      "Predicted: 4562.93 Expected: 4567.18 Difference: 4.25\n",
      "Predicted: 5274.38 Expected: 5266.95 Difference: -7.430000000000291\n",
      "Predicted: 4216.29 Expected: 4237.86 Difference: 21.56999999999971\n",
      "Predicted: 4555.71 Expected: 4559.34 Difference: 3.630000000000109\n",
      "Predicted: 4563.02 Expected: 4567.46 Difference: 4.4399999999996\n",
      "Predicted: 4749.61 Expected: 4698.35 Difference: -51.25999999999931\n",
      "Predicted: 5297.13 Expected: 5303.27 Difference: 6.140000000000327\n",
      "Predicted: 5012.11 Expected: 5029.73 Difference: 17.61999999999989\n",
      "Predicted: 4569.9 Expected: 4550.58 Difference: -19.31999999999971\n",
      "Predicted: 5144.51 Expected: 5175.27 Difference: 30.76000000000022\n",
      "Predicted: 5247.91 Expected: 5235.48 Difference: -12.430000000000291\n",
      "Predicted: 4457.91 Expected: 4457.49 Difference: -0.42000000000007276\n",
      "Predicted: 4365.87 Expected: 4349.61 Difference: -16.26000000000022\n",
      "Predicted: 4467.73 Expected: 4489.72 Difference: 21.99000000000069\n",
      "Predicted: 4534.16 Expected: 4554.98 Difference: 20.81999999999971\n",
      "Predicted: 4380.34 Expected: 4415.24 Difference: 34.899999999999636\n",
      "Predicted: 5175.71 Expected: 5187.67 Difference: 11.960000000000036\n",
      "Predicted: 5194.62 Expected: 5224.62 Difference: 30.0\n",
      "Predicted: 5024.92 Expected: 5048.42 Difference: 23.5\n",
      "Predicted: 4586.94 Expected: 4604.37 Difference: 17.43000000000029\n",
      "Predicted: 4361.53 Expected: 4365.98 Difference: 4.449999999999818\n",
      "Predicted: 4465.54 Expected: 4467.44 Difference: 1.8999999999996362\n",
      "Predicted: 4451.16 Expected: 4453.53 Difference: 2.369999999999891\n",
      "Predicted: 4887.01 Expected: 4868.55 Difference: -18.460000000000036\n",
      "Predicted: 4464.55 Expected: 4437.86 Difference: -26.69000000000051\n",
      "Predicted: 4923.96 Expected: 4924.97 Difference: 1.0100000000002183\n",
      "Predicted: 4437.92 Expected: 4443.95 Difference: 6.029999999999745\n",
      "Predicted: 4704.76 Expected: 4688.68 Difference: -16.079999999999927\n",
      "Predicted: 4506.55 Expected: 4496.83 Difference: -9.720000000000255\n",
      "Predicted: 4875.33 Expected: 4906.19 Difference: 30.859999999999673\n",
      "Predicted: 4944.71 Expected: 4942.81 Difference: -1.8999999999996362\n",
      "Predicted: 4406.4 Expected: 4387.55 Difference: -18.849999999999454\n",
      "Predicted: 4440.13 Expected: 4402.2 Difference: -37.93000000000029\n",
      "Predicted: 4415.02 Expected: 4398.95 Difference: -16.07000000000062\n",
      "Predicted: 4553.74 Expected: 4550.43 Difference: -3.3099999999994907\n",
      "Predicted: 4472.76 Expected: 4461.9 Difference: -10.860000000000582\n",
      "Predicted: 4786.2 Expected: 4783.35 Difference: -2.8499999999994543\n",
      "Predicted: 5109.1 Expected: 5116.17 Difference: 7.069999999999709\n",
      "Predicted: 4577.15 Expected: 4576.73 Difference: -0.42000000000007276\n",
      "Predicted: 4139.57 Expected: 4117.37 Difference: -22.199999999999818\n",
      "Predicted: 4903.83 Expected: 4927.93 Difference: 24.100000000000364\n",
      "Predicted: 5135.92 Expected: 5130.95 Difference: -4.970000000000255\n",
      "Predicted: 5011.6 Expected: 5026.61 Difference: 15.009999999999309\n",
      "Predicted: 4499.66 Expected: 4510.04 Difference: 10.38000000000011\n",
      "Predicted: 4408.03 Expected: 4411.55 Difference: 3.5200000000004366\n",
      "Predicted: 4364.31 Expected: 4376.95 Difference: 12.639999999999418\n",
      "Predicted: 4505.66 Expected: 4514.87 Difference: 9.210000000000036\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(x_train):\n",
    "        y_val = model.forward(data)\n",
    "        print(f\"Predicted: {round(y_val.item(),2)} Expected: {round(y_train[i].item(),2)} Difference: {round(y_train[i].item(),2) - round(y_val.item(),2)}\")\n",
    "        if round(y_val.item(),2) == round(y_train[i].item(),2):\n",
    "            correct += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
