{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessica/miniconda3/envs/torch-test/lib/python3.12/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 972.1571655273438\n",
      "199 651.276611328125\n",
      "299 437.50341796875\n",
      "399 295.0176086425781\n",
      "499 200.00001525878906\n",
      "599 136.6045379638672\n",
      "699 94.28449249267578\n",
      "799 66.01742553710938\n",
      "899 47.12554168701172\n",
      "999 34.49162292480469\n",
      "1099 26.037155151367188\n",
      "1199 20.375654220581055\n",
      "1299 16.581735610961914\n",
      "1399 14.0374755859375\n",
      "1499 12.32996940612793\n",
      "1599 11.183087348937988\n",
      "1699 10.412118911743164\n",
      "1799 9.893401145935059\n",
      "1899 9.54409408569336\n",
      "1999 9.308650016784668\n",
      "Result: y = 0.013585366308689117 + 0.8391690254211426 x + -0.002343702595680952 x^2 + -0.09083095192909241 x^3\n"
     ]
    }
   ],
   "source": [
    "# Code from \n",
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 232 entries, 0 to 231\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Date    232 non-null    object\n",
      " 1   Open    232 non-null    object\n",
      " 2   High    232 non-null    object\n",
      " 3   Low     232 non-null    object\n",
      " 4   Close   232 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 9.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"stock-data.csv\")\n",
    "df.info()\n",
    "df.drop(\"Date\", inplace=True, axis = 1)\n",
    "df = df.replace(\",\",\"\", regex = True)\n",
    "df = df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 232 entries, 0 to 231\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Open    232 non-null    float64\n",
      " 1   High    232 non-null    float64\n",
      " 2   Low     232 non-null    float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 5.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "features = df[[\"Open\", \"High\", \"Low\"]]\n",
    "print(features.info())\n",
    "features = features.values\n",
    "target = df[[\"Close\"]].values\n",
    "# features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessica/miniconda3/envs/torch-test/lib/python3.12/site-packages/torch/utils/_device.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# mean = features.mean(axis = 0)\n",
    "# std = features.std(axis=0)\n",
    "# features = (features - mean)/std\n",
    "\n",
    "features = torch.round(torch.tensor(features, dtype = torch.float64)*100/100)\n",
    "\n",
    "#this is beacuse we want a 1D array for our answerers\n",
    "target = torch.round(torch.tensor(target, dtype = torch.float64).view(-1,1)*100)/100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class stockPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(stockPredictor, self).__init__()\n",
    "        #this data has 4 inputs than we want to go to 64 i dont not understand why we chose these numbers\n",
    "        self.fc1 = nn.Linear(3,63)\n",
    "        self.fc2 = nn.Linear(63,32)\n",
    "        self.fc3 = nn.Linear(32,1)\n",
    "        self.double()\n",
    "    \n",
    "    #goes through each layer of the neural netwrok and returns an answer depending\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = stockPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4536.3400],\n",
      "        [5234.1800],\n",
      "        [4754.6300],\n",
      "        [5307.0100],\n",
      "        [5070.5500],\n",
      "        [5071.6300],\n",
      "        [5204.3400],\n",
      "        [5178.5100],\n",
      "        [4845.6500],\n",
      "        [5243.7700],\n",
      "        [4719.1900],\n",
      "        [4585.5900],\n",
      "        [4890.9700],\n",
      "        [4411.5900],\n",
      "        [4643.7000],\n",
      "        [4193.8000],\n",
      "        [4508.2400],\n",
      "        [5199.0600],\n",
      "        [5308.1500],\n",
      "        [4522.7900],\n",
      "        [5104.7600],\n",
      "        [4997.9100],\n",
      "        [4186.7700],\n",
      "        [4373.6300],\n",
      "        [5149.4200],\n",
      "        [4247.6800],\n",
      "        [4465.4800],\n",
      "        [4376.3100],\n",
      "        [5137.0800],\n",
      "        [5203.5800],\n",
      "        [4274.5100],\n",
      "        [5087.0300],\n",
      "        [4468.8300],\n",
      "        [4505.4200],\n",
      "        [5096.2700],\n",
      "        [4780.9400],\n",
      "        [4314.6000],\n",
      "        [4472.1600],\n",
      "        [4569.7800],\n",
      "        [5308.1300],\n",
      "        [4451.1400],\n",
      "        [4769.8300],\n",
      "        [4768.3700],\n",
      "        [4507.6600],\n",
      "        [5035.6900],\n",
      "        [4478.0300],\n",
      "        [4399.7700]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = 41)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500], Loss: 111519.7828668478\n",
      "Epoch [20/500], Loss: 25215.477989263854\n",
      "Epoch [30/500], Loss: 3393.6843144882705\n",
      "Epoch [40/500], Loss: 649.8443699380229\n",
      "Epoch [50/500], Loss: 2183.4645862531365\n",
      "Epoch [60/500], Loss: 308.53114472245045\n",
      "Epoch [70/500], Loss: 533.5566027708862\n",
      "Epoch [80/500], Loss: 304.84162114806094\n",
      "Epoch [90/500], Loss: 270.7419904949941\n",
      "Epoch [100/500], Loss: 272.8051579048661\n",
      "Epoch [110/500], Loss: 271.89562070503297\n",
      "Epoch [120/500], Loss: 270.89503798070524\n",
      "Epoch [130/500], Loss: 270.55972638699893\n",
      "Epoch [140/500], Loss: 270.5529601175776\n",
      "Epoch [150/500], Loss: 270.5309238044288\n",
      "Epoch [160/500], Loss: 270.43254453064094\n",
      "Epoch [170/500], Loss: 270.36780390241296\n",
      "Epoch [180/500], Loss: 270.3194470295251\n",
      "Epoch [190/500], Loss: 270.26363679601116\n",
      "Epoch [200/500], Loss: 270.21000010242335\n",
      "Epoch [210/500], Loss: 270.15411970331843\n",
      "Epoch [220/500], Loss: 270.09707631587577\n",
      "Epoch [230/500], Loss: 270.038625370523\n",
      "Epoch [240/500], Loss: 269.978724583465\n",
      "Epoch [250/500], Loss: 269.91742364174115\n",
      "Epoch [260/500], Loss: 269.8547378043223\n",
      "Epoch [270/500], Loss: 269.79068166180974\n",
      "Epoch [280/500], Loss: 269.7252715676813\n",
      "Epoch [290/500], Loss: 269.65852331303404\n",
      "Epoch [300/500], Loss: 269.59045169365044\n",
      "Epoch [310/500], Loss: 269.5210705728565\n",
      "Epoch [320/500], Loss: 269.45039296729317\n",
      "Epoch [330/500], Loss: 269.37843111684\n",
      "Epoch [340/500], Loss: 269.30519654446834\n",
      "Epoch [350/500], Loss: 269.23070010902535\n",
      "Epoch [360/500], Loss: 269.1549520531231\n",
      "Epoch [370/500], Loss: 269.0779620473246\n",
      "Epoch [380/500], Loss: 268.9997392298332\n",
      "Epoch [390/500], Loss: 268.92029224196676\n",
      "Epoch [400/500], Loss: 268.83962926059684\n",
      "Epoch [410/500], Loss: 268.757758027732\n",
      "Epoch [420/500], Loss: 268.6746858774324\n",
      "Epoch [430/500], Loss: 268.59041976050105\n",
      "Epoch [440/500], Loss: 268.50496626706956\n",
      "Epoch [450/500], Loss: 268.4183316473967\n",
      "Epoch [460/500], Loss: 268.3305218309535\n",
      "Epoch [470/500], Loss: 268.2415424440795\n",
      "Epoch [480/500], Loss: 268.15139882624914\n",
      "Epoch [490/500], Loss: 268.06009604516106\n",
      "Epoch [500/500], Loss: 267.96763891070265\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "epochs = 500\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    y_pred = model.forward(x_train)\n",
    "\n",
    "    outputs = model(features)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    losses.append(loss.detach().numpy())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch +1) %10 ==0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_eval = model.forward(x_test)\n",
    "    loss = criterion(y_eval,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 4547.17 Expected: 4536.34\n",
      "Predicted: 5239.75 Expected: 5234.18\n",
      "Predicted: 4756.61 Expected: 4754.63\n",
      "Predicted: 5306.89 Expected: 5307.01\n",
      "Predicted: 5052.82 Expected: 5070.55\n",
      "Predicted: 5070.49 Expected: 5071.63\n",
      "Predicted: 5190.16 Expected: 5204.34\n",
      "Predicted: 5157.09 Expected: 5178.51\n",
      "Predicted: 4878.88 Expected: 4845.65\n",
      "Predicted: 5248.73 Expected: 5243.77\n",
      "Predicted: 4716.96 Expected: 4719.19\n",
      "Predicted: 4579.15 Expected: 4585.59\n",
      "Predicted: 4895.38 Expected: 4890.97\n",
      "Predicted: 4406.42 Expected: 4411.59\n",
      "Predicted: 4627.28 Expected: 4643.7\n",
      "Predicted: 4175.95 Expected: 4193.8\n",
      "Predicted: 4501.38 Expected: 4508.24\n",
      "Predicted: 5177.43 Expected: 5199.06\n",
      "Predicted: 5288.3 Expected: 5308.15\n",
      "Predicted: 4520.11 Expected: 4522.79\n",
      "Predicted: 5111.63 Expected: 5104.76\n",
      "Predicted: 4995.08 Expected: 4997.91\n",
      "Predicted: 4209.27 Expected: 4186.77\n",
      "Predicted: 4363.23 Expected: 4373.63\n",
      "Predicted: 5161.95 Expected: 5149.42\n",
      "Predicted: 4240.46 Expected: 4247.68\n",
      "Predicted: 4468.73 Expected: 4465.48\n",
      "Predicted: 4420.6 Expected: 4376.31\n",
      "Predicted: 5118.03 Expected: 5137.08\n",
      "Predicted: 5221.14 Expected: 5203.58\n",
      "Predicted: 4267.95 Expected: 4274.51\n",
      "Predicted: 5067.18 Expected: 5087.03\n",
      "Predicted: 4494.13 Expected: 4468.83\n",
      "Predicted: 4515.58 Expected: 4505.42\n",
      "Predicted: 5085.34 Expected: 5096.27\n",
      "Predicted: 4765.08 Expected: 4780.94\n",
      "Predicted: 4336.76 Expected: 4314.6\n",
      "Predicted: 4476.69 Expected: 4472.16\n",
      "Predicted: 4561.22 Expected: 4569.78\n",
      "Predicted: 5314.79 Expected: 5308.13\n",
      "Predicted: 4445.14 Expected: 4451.14\n",
      "Predicted: 4772.22 Expected: 4769.83\n",
      "Predicted: 4757.52 Expected: 4768.37\n",
      "Predicted: 4520.91 Expected: 4507.66\n",
      "Predicted: 5076.38 Expected: 5035.69\n",
      "Predicted: 4509.62 Expected: 4478.03\n",
      "Predicted: 4385.51 Expected: 4399.77\n",
      "Correct: 0\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(x_test):\n",
    "        y_val = model.forward(data)\n",
    "        print(f\"Predicted: {round(y_val.item(),2)} Expected: {round(y_test[i].item(),2)}\")\n",
    "        if round(y_val.item(),2) == round(y_test[i].item(),2):\n",
    "            num_correct += 1\n",
    "print(f\"Correct: {num_correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 4557.86 Expected: 4556.62\n",
      "Predicted: 5121.87 Expected: 5127.79\n",
      "Predicted: 4363.18 Expected: 4358.24\n",
      "Predicted: 4366.84 Expected: 4373.2\n",
      "Predicted: 4774.57 Expected: 4783.45\n",
      "Predicted: 4248.64 Expected: 4258.19\n",
      "Predicted: 4428.94 Expected: 4433.31\n",
      "Predicted: 5210.24 Expected: 5202.39\n",
      "Predicted: 4740.39 Expected: 4742.83\n",
      "Predicted: 4555.15 Expected: 4567.8\n",
      "Predicted: 4570.95 Expected: 4549.34\n",
      "Predicted: 4531.15 Expected: 4513.39\n",
      "Predicted: 5144.79 Expected: 5123.41\n",
      "Predicted: 4778.32 Expected: 4781.58\n",
      "Predicted: 5122.31 Expected: 5117.09\n",
      "Predicted: 4578.06 Expected: 4582.23\n",
      "Predicted: 5044.22 Expected: 5064.2\n",
      "Predicted: 4223.76 Expected: 4217.04\n",
      "Predicted: 4535.66 Expected: 4538.19\n",
      "Predicted: 5155.97 Expected: 5123.69\n",
      "Predicted: 4346.98 Expected: 4327.78\n",
      "Predicted: 5097.88 Expected: 5088.8\n",
      "Predicted: 5301.0 Expected: 5306.04\n",
      "Predicted: 4428.97 Expected: 4404.33\n",
      "Predicted: 4475.32 Expected: 4450.32\n",
      "Predicted: 5069.05 Expected: 5069.76\n",
      "Predicted: 4885.27 Expected: 4894.16\n",
      "Predicted: 4553.11 Expected: 4554.64\n",
      "Predicted: 5196.0 Expected: 5209.91\n",
      "Predicted: 4354.75 Expected: 4358.34\n",
      "Predicted: 4506.47 Expected: 4518.44\n",
      "Predicted: 4739.03 Expected: 4740.56\n",
      "Predicted: 5191.01 Expected: 5187.7\n",
      "Predicted: 4585.06 Expected: 4588.96\n",
      "Predicted: 5280.27 Expected: 5291.34\n",
      "Predicted: 5160.66 Expected: 5160.64\n",
      "Predicted: 4566.34 Expected: 4566.75\n",
      "Predicted: 5271.26 Expected: 5283.4\n",
      "Predicted: 4503.71 Expected: 4501.89\n",
      "Predicted: 4718.41 Expected: 4719.55\n",
      "Predicted: 5061.96 Expected: 5051.41\n",
      "Predicted: 4677.07 Expected: 4707.09\n",
      "Predicted: 4395.74 Expected: 4370.36\n",
      "Predicted: 4359.58 Expected: 4369.71\n",
      "Predicted: 4948.23 Expected: 4954.23\n",
      "Predicted: 4568.84 Expected: 4565.72\n",
      "Predicted: 4251.22 Expected: 4229.45\n",
      "Predicted: 4496.76 Expected: 4505.1\n",
      "Predicted: 4158.56 Expected: 4137.23\n",
      "Predicted: 5021.72 Expected: 5005.57\n",
      "Predicted: 5205.68 Expected: 5147.21\n",
      "Predicted: 4272.52 Expected: 4308.5\n",
      "Predicted: 4702.71 Expected: 4697.24\n",
      "Predicted: 4307.53 Expected: 4278.0\n",
      "Predicted: 4402.66 Expected: 4409.53\n",
      "Predicted: 5212.48 Expected: 5211.49\n",
      "Predicted: 4989.23 Expected: 4967.23\n",
      "Predicted: 5252.7 Expected: 5241.53\n",
      "Predicted: 4571.25 Expected: 4537.41\n",
      "Predicted: 4485.75 Expected: 4499.38\n",
      "Predicted: 4306.78 Expected: 4288.05\n",
      "Predicted: 5198.86 Expected: 5214.08\n",
      "Predicted: 5303.4 Expected: 5267.84\n",
      "Predicted: 5148.1 Expected: 5157.36\n",
      "Predicted: 4716.54 Expected: 4704.81\n",
      "Predicted: 5197.98 Expected: 5205.81\n",
      "Predicted: 4427.16 Expected: 4439.26\n",
      "Predicted: 4252.74 Expected: 4224.16\n",
      "Predicted: 5088.97 Expected: 5078.65\n",
      "Predicted: 5033.93 Expected: 5021.84\n",
      "Predicted: 5095.38 Expected: 5099.96\n",
      "Predicted: 5031.47 Expected: 5011.12\n",
      "Predicted: 5223.95 Expected: 5218.19\n",
      "Predicted: 4787.37 Expected: 4783.83\n",
      "Predicted: 5238.47 Expected: 5277.51\n",
      "Predicted: 4555.67 Expected: 4554.89\n",
      "Predicted: 5085.57 Expected: 5069.53\n",
      "Predicted: 4245.22 Expected: 4263.75\n",
      "Predicted: 5226.69 Expected: 5222.68\n",
      "Predicted: 5226.05 Expected: 5221.42\n",
      "Predicted: 5152.95 Expected: 5150.48\n",
      "Predicted: 5312.16 Expected: 5321.41\n",
      "Predicted: 4609.42 Expected: 4622.44\n",
      "Predicted: 5296.14 Expected: 5304.72\n",
      "Predicted: 4371.16 Expected: 4347.35\n",
      "Predicted: 4322.14 Expected: 4337.44\n",
      "Predicted: 4282.3 Expected: 4288.39\n",
      "Predicted: 4729.99 Expected: 4746.75\n",
      "Predicted: 4942.34 Expected: 4958.61\n",
      "Predicted: 4731.5 Expected: 4739.21\n",
      "Predicted: 4771.83 Expected: 4780.24\n",
      "Predicted: 5327.41 Expected: 5354.03\n",
      "Predicted: 5109.88 Expected: 5117.94\n",
      "Predicted: 4377.41 Expected: 4382.78\n",
      "Predicted: 5055.49 Expected: 5018.39\n",
      "Predicted: 5045.64 Expected: 5022.21\n",
      "Predicted: 4371.84 Expected: 4378.38\n",
      "Predicted: 4523.07 Expected: 4515.77\n",
      "Predicted: 4766.47 Expected: 4765.98\n",
      "Predicted: 4733.16 Expected: 4763.54\n",
      "Predicted: 4461.18 Expected: 4464.05\n",
      "Predicted: 4313.71 Expected: 4335.66\n",
      "Predicted: 5233.0 Expected: 5248.49\n",
      "Predicted: 4985.66 Expected: 4995.06\n",
      "Predicted: 4981.13 Expected: 5000.62\n",
      "Predicted: 4509.39 Expected: 4502.88\n",
      "Predicted: 4577.91 Expected: 4594.63\n",
      "Predicted: 5070.87 Expected: 5078.18\n",
      "Predicted: 5256.79 Expected: 5254.35\n",
      "Predicted: 4481.06 Expected: 4487.46\n",
      "Predicted: 4534.27 Expected: 4547.38\n",
      "Predicted: 4484.03 Expected: 4467.71\n",
      "Predicted: 4484.64 Expected: 4495.7\n",
      "Predicted: 4294.57 Expected: 4317.78\n",
      "Predicted: 4338.35 Expected: 4320.06\n",
      "Predicted: 4856.64 Expected: 4864.6\n",
      "Predicted: 5114.1 Expected: 5061.82\n",
      "Predicted: 4388.88 Expected: 4405.71\n",
      "Predicted: 5235.15 Expected: 5246.68\n",
      "Predicted: 5312.18 Expected: 5297.1\n",
      "Predicted: 4355.09 Expected: 4330.0\n",
      "Predicted: 4966.14 Expected: 4981.8\n",
      "Predicted: 4948.28 Expected: 4953.17\n",
      "Predicted: 4466.44 Expected: 4497.63\n",
      "Predicted: 5167.48 Expected: 5165.31\n",
      "Predicted: 4748.91 Expected: 4756.5\n",
      "Predicted: 4976.84 Expected: 4975.51\n",
      "Predicted: 4511.48 Expected: 4514.02\n",
      "Predicted: 4772.54 Expected: 4774.75\n",
      "Predicted: 4815.05 Expected: 4839.81\n",
      "Predicted: 5162.41 Expected: 5180.74\n",
      "Predicted: 5005.24 Expected: 5010.6\n",
      "Predicted: 4857.44 Expected: 4850.43\n",
      "Predicted: 4548.44 Expected: 4534.87\n",
      "Predicted: 4292.17 Expected: 4273.53\n",
      "Predicted: 4420.16 Expected: 4436.01\n",
      "Predicted: 4155.91 Expected: 4166.82\n",
      "Predicted: 4291.32 Expected: 4299.7\n",
      "Predicted: 4566.68 Expected: 4567.18\n",
      "Predicted: 5274.39 Expected: 5266.95\n",
      "Predicted: 4222.74 Expected: 4237.86\n",
      "Predicted: 4557.86 Expected: 4559.34\n",
      "Predicted: 4567.56 Expected: 4567.46\n",
      "Predicted: 4741.17 Expected: 4698.35\n",
      "Predicted: 5296.5 Expected: 5303.27\n",
      "Predicted: 5017.13 Expected: 5029.73\n",
      "Predicted: 4569.35 Expected: 4550.58\n",
      "Predicted: 5148.41 Expected: 5175.27\n",
      "Predicted: 5243.58 Expected: 5235.48\n",
      "Predicted: 4462.08 Expected: 4457.49\n",
      "Predicted: 4358.38 Expected: 4349.61\n",
      "Predicted: 4472.52 Expected: 4489.72\n",
      "Predicted: 4539.49 Expected: 4554.98\n",
      "Predicted: 4386.39 Expected: 4415.24\n",
      "Predicted: 5180.22 Expected: 5187.67\n",
      "Predicted: 5200.15 Expected: 5224.62\n",
      "Predicted: 5026.26 Expected: 5048.42\n",
      "Predicted: 4592.44 Expected: 4604.37\n",
      "Predicted: 4361.65 Expected: 4365.98\n",
      "Predicted: 4467.86 Expected: 4467.44\n",
      "Predicted: 4455.11 Expected: 4453.53\n",
      "Predicted: 4886.87 Expected: 4868.55\n",
      "Predicted: 4458.2 Expected: 4437.86\n",
      "Predicted: 4925.12 Expected: 4924.97\n",
      "Predicted: 4435.56 Expected: 4443.95\n",
      "Predicted: 4708.72 Expected: 4688.68\n",
      "Predicted: 4506.67 Expected: 4496.83\n",
      "Predicted: 4881.47 Expected: 4906.19\n",
      "Predicted: 4940.05 Expected: 4942.81\n",
      "Predicted: 4403.19 Expected: 4387.55\n",
      "Predicted: 4433.74 Expected: 4402.2\n",
      "Predicted: 4419.56 Expected: 4398.95\n",
      "Predicted: 4555.01 Expected: 4550.43\n",
      "Predicted: 4473.59 Expected: 4461.9\n",
      "Predicted: 4788.42 Expected: 4783.35\n",
      "Predicted: 5108.05 Expected: 5116.17\n",
      "Predicted: 4578.07 Expected: 4576.73\n",
      "Predicted: 4133.14 Expected: 4117.37\n",
      "Predicted: 4909.08 Expected: 4927.93\n",
      "Predicted: 5139.8 Expected: 5130.95\n",
      "Predicted: 5016.18 Expected: 5026.61\n",
      "Predicted: 4504.07 Expected: 4510.04\n",
      "Predicted: 4409.48 Expected: 4411.55\n",
      "Predicted: 4363.78 Expected: 4376.95\n",
      "Predicted: 4509.2 Expected: 4514.87\n",
      "Correct: 0\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(x_train):\n",
    "        y_val = model.forward(data)\n",
    "        print(f\"Predicted: {round(y_val.item(),2)} Expected: {round(y_train[i].item(),2)}\")\n",
    "        if round(y_val.item(),2) == round(y_train[i].item(),2):\n",
    "            correct += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
