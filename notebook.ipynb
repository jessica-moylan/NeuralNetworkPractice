{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessica/miniconda3/envs/torch-test/lib/python3.12/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2050.810791015625\n",
      "199 1369.9517822265625\n",
      "299 916.5589599609375\n",
      "399 614.503662109375\n",
      "499 413.1788330078125\n",
      "599 278.9266357421875\n",
      "699 189.35536193847656\n",
      "799 129.5626983642578\n",
      "899 89.62590026855469\n",
      "999 62.93535614013672\n",
      "1099 45.08642578125\n",
      "1199 33.14250564575195\n",
      "1299 25.14455223083496\n",
      "1399 19.785064697265625\n",
      "1499 16.19097900390625\n",
      "1599 13.77891731262207\n",
      "1699 12.158859252929688\n",
      "1799 11.069828987121582\n",
      "1899 10.337150573730469\n",
      "1999 9.843790054321289\n",
      "Result: y = -0.019176112487912178 + 0.8824307322502136 x + 0.003308200975880027 x^2 + -0.09698455035686493 x^3\n"
     ]
    }
   ],
   "source": [
    "# Code from \n",
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 232 entries, 0 to 231\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Date    232 non-null    object\n",
      " 1   Open    232 non-null    object\n",
      " 2   High    232 non-null    object\n",
      " 3   Low     232 non-null    object\n",
      " 4   Close   232 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 9.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"stock-data.csv\")\n",
    "df.info()\n",
    "df.drop(\"Date\", inplace=True, axis = 1)\n",
    "df = df.replace(\",\",\"\", regex = True)\n",
    "df = df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Open         High          Low\n",
      "count   232.000000   232.000000   232.000000\n",
      "mean   4746.149655  4766.391207  4725.216595\n",
      "std     344.244756   343.212711   343.746747\n",
      "min    4139.390000  4156.700000  4103.780000\n",
      "25%    4457.387500  4479.260000  4443.580000\n",
      "50%    4708.965000  4728.035000  4696.080000\n",
      "75%    5087.270000  5106.450000  5057.555000\n",
      "max    5340.260000  5354.160000  5302.400000\n"
     ]
    }
   ],
   "source": [
    "features = df[[\"Open\", \"High\", \"Low\"]]\n",
    "features = features.values\n",
    "target = df[[\"Close\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2988935/4108026271.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target['Close'] = scaler.fit_transform(target['Close'].values.reshape(-1,1))\n"
     ]
    }
   ],
   "source": [
    "#Ww want to normalize our data to between -1 and 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "target['Close'] = scaler.fit_transform(target['Close'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5314.4800, 5354.1600, 5297.6400],\n",
      "        [5278.2400, 5298.8000, 5257.6300],\n",
      "        [5297.1500, 5302.1100, 5234.3200],\n",
      "        [5243.2100, 5280.3300, 5191.6800],\n",
      "        [5259.7700, 5260.2100, 5222.1000],\n",
      "        [5278.7300, 5282.2700, 5262.7000],\n",
      "        [5315.9100, 5315.9100, 5280.8900],\n",
      "        [5281.4500, 5311.6500, 5278.3900],\n",
      "        [5340.2600, 5341.8800, 5256.9300],\n",
      "        [5319.2800, 5323.1800, 5286.0100],\n",
      "        [5298.6900, 5324.3200, 5297.8700],\n",
      "        [5305.3500, 5325.3200, 5302.4000],\n",
      "        [5303.1000, 5305.4500, 5283.5900],\n",
      "        [5310.0700, 5325.4900, 5296.1900],\n",
      "        [5263.2600, 5311.7600, 5263.2600],\n",
      "        [5221.1000, 5250.3700, 5217.9800],\n",
      "        [5233.0800, 5237.2600, 5211.1600],\n",
      "        [5225.4900, 5239.6600, 5209.6800],\n",
      "        [5189.0300, 5215.3000, 5180.4100],\n",
      "        [5168.9800, 5191.9500, 5165.8600],\n",
      "        [5187.2000, 5200.2300, 5178.9600],\n",
      "        [5142.4200, 5181.0000, 5142.4200],\n",
      "        [5122.7800, 5139.1200, 5101.2200],\n",
      "        [5049.3200, 5073.2100, 5011.0500],\n",
      "        [5029.0300, 5096.1200, 5013.4500],\n",
      "        [5103.7800, 5110.8300, 5035.3100],\n",
      "        [5114.1300, 5123.4900, 5088.6500],\n",
      "        [5084.6500, 5114.6200, 5073.1400],\n",
      "        [5019.8800, 5057.7500, 4990.5800],\n",
      "        [5084.8600, 5089.4800, 5047.0200],\n",
      "        [5028.8500, 5076.1200, 5027.9600],\n",
      "        [4987.3300, 5038.8400, 4969.4000],\n",
      "        [5005.4400, 5019.0200, 4953.5600],\n",
      "        [5031.5200, 5056.6600, 5001.8900],\n",
      "        [5068.9700, 5077.9600, 5007.2500],\n",
      "        [5064.5900, 5079.8400, 5039.8300],\n",
      "        [5149.6700, 5168.4300, 5052.4700],\n",
      "        [5171.5100, 5175.0300, 5107.9400],\n",
      "        [5172.9500, 5211.7800, 5138.7700],\n",
      "        [5167.8800, 5178.4300, 5138.7000],\n",
      "        [5217.0300, 5224.8100, 5160.7800],\n",
      "        [5211.3700, 5219.5700, 5197.3500],\n",
      "        [5158.9500, 5222.1800, 5157.2100],\n",
      "        [5244.0500, 5256.5900, 5146.0600],\n",
      "        [5194.3700, 5228.7500, 5194.3700],\n",
      "        [5204.2900, 5208.3400, 5184.0500],\n",
      "        [5257.9700, 5263.9500, 5229.2000],\n",
      "        [5248.0300, 5264.8500, 5245.8200],\n",
      "        [5226.3100, 5249.2600, 5213.9200],\n",
      "        [5228.8500, 5235.1600, 5203.4200],\n",
      "        [5219.5200, 5229.0900, 5216.0900],\n",
      "        [5242.4800, 5246.0900, 5229.8700],\n",
      "        [5253.4300, 5261.1000, 5240.6600],\n",
      "        [5181.6900, 5226.1900, 5171.5500],\n",
      "        [5139.0900, 5180.3100, 5131.5900],\n",
      "        [5154.7700, 5175.6000, 5145.4700],\n",
      "        [5123.3100, 5136.8600, 5104.3500],\n",
      "        [5175.1400, 5176.8500, 5123.3000],\n",
      "        [5173.4900, 5179.1400, 5151.8800],\n",
      "        [5134.3000, 5179.8700, 5114.4800],\n",
      "        [5111.9600, 5124.6600, 5091.1400],\n",
      "        [5164.4600, 5189.2600, 5117.5000],\n",
      "        [5132.3800, 5165.6200, 5128.2100],\n",
      "        [5108.0300, 5127.9700, 5092.2200],\n",
      "        [5110.5200, 5114.5400, 5056.8200],\n",
      "        [5130.9900, 5149.6700, 5127.1800],\n",
      "        [5098.5100, 5140.3300, 5094.1600],\n",
      "        [5085.3600, 5104.9900, 5061.8900],\n",
      "        [5067.2000, 5077.3700, 5058.3500],\n",
      "        [5074.6000, 5080.6900, 5057.2900],\n",
      "        [5093.0000, 5097.6600, 5068.9100],\n",
      "        [5100.9200, 5111.0600, 5081.4600],\n",
      "        [5038.8300, 5094.3900, 5038.8300],\n",
      "        [4963.0300, 4983.2100, 4946.0000],\n",
      "        [4989.3200, 4993.7100, 4955.0200],\n",
      "        [5031.1300, 5038.7000, 4999.5200],\n",
      "        [5003.1400, 5032.7200, 4999.4400],\n",
      "        [4976.4400, 5002.5200, 4956.4500],\n",
      "        [4967.9400, 4971.3000, 4920.3100],\n",
      "        [5026.8300, 5048.3900, 5016.8300],\n",
      "        [5004.1700, 5030.0600, 5000.3400],\n",
      "        [4995.1600, 5000.4000, 4987.0900],\n",
      "        [4973.0500, 4999.8900, 4969.0500],\n",
      "        [4950.1600, 4957.7700, 4934.8800],\n",
      "        [4957.1900, 4957.1900, 4918.0900],\n",
      "        [4916.0600, 4975.2900, 4907.9900],\n",
      "        [4861.1100, 4906.9700, 4853.5200],\n",
      "        [4899.1900, 4906.7500, 4845.1500],\n",
      "        [4925.8900, 4931.0900, 4916.2700],\n",
      "        [4892.9500, 4929.3100, 4887.4000],\n",
      "        [4888.9100, 4906.6900, 4881.4700],\n",
      "        [4886.6600, 4898.1500, 4869.3400],\n",
      "        [4888.5600, 4903.6800, 4865.9400],\n",
      "        [4856.8000, 4866.4800, 4844.3700],\n",
      "        [4853.4200, 4868.4100, 4844.0500],\n",
      "        [4796.2800, 4842.0700, 4785.8700],\n",
      "        [4760.1000, 4785.7900, 4740.5700],\n",
      "        [4739.1300, 4744.2300, 4714.8200],\n",
      "        [4772.3500, 4782.3400, 4747.1200],\n",
      "        [4791.1800, 4802.4000, 4768.9800],\n",
      "        [4792.1300, 4798.5000, 4739.5800],\n",
      "        [4759.9400, 4790.8000, 4756.2000],\n",
      "        [4741.9300, 4765.4700, 4730.3500],\n",
      "        [4703.7000, 4764.5400, 4699.8200],\n",
      "        [4690.5700, 4721.4900, 4682.1100],\n",
      "        [4697.4200, 4726.7800, 4687.5300],\n",
      "        [4725.0700, 4729.2900, 4699.7100],\n",
      "        [4745.2000, 4754.3300, 4722.6700],\n",
      "        [4782.8800, 4788.4300, 4751.9900],\n",
      "        [4786.4400, 4793.3000, 4780.9800],\n",
      "        [4773.4500, 4785.3900, 4768.9000],\n",
      "        [4758.8600, 4784.7200, 4758.4500],\n",
      "        [4753.9200, 4772.9400, 4736.7700],\n",
      "        [4724.2900, 4748.7100, 4708.3500],\n",
      "        [4764.7300, 4778.0100, 4697.8200],\n",
      "        [4743.7200, 4768.6900, 4743.7200],\n",
      "        [4725.5800, 4749.5200, 4725.5800],\n",
      "        [4714.2300, 4725.5300, 4704.6900],\n",
      "        [4721.0400, 4738.5700, 4694.3400],\n",
      "        [4646.2000, 4709.6900, 4643.2300],\n",
      "        [4618.3000, 4643.9300, 4608.0900],\n",
      "        [4593.3900, 4623.7100, 4593.3900],\n",
      "        [4576.2000, 4609.2300, 4574.0600],\n",
      "        [4568.8400, 4590.9200, 4565.2200],\n",
      "        [4586.2300, 4590.7400, 4546.5000],\n",
      "        [4557.2500, 4578.5600, 4551.6800],\n",
      "        [4564.3700, 4572.3700, 4546.7200],\n",
      "        [4559.4300, 4599.3900, 4554.7100],\n",
      "        [4554.8700, 4569.8900, 4537.2400],\n",
      "        [4571.8400, 4587.6400, 4547.1500],\n",
      "        [4545.5500, 4568.1400, 4540.5100],\n",
      "        [4554.8600, 4560.5200, 4546.3200],\n",
      "        [4555.8400, 4560.3100, 4552.8000],\n",
      "        [4553.0400, 4568.4300, 4545.0500],\n",
      "        [4538.7700, 4542.1400, 4525.5100],\n",
      "        [4511.7000, 4557.1100, 4510.3600],\n",
      "        [4509.5500, 4520.1200, 4499.6600],\n",
      "        [4497.0800, 4511.9900, 4487.8300],\n",
      "        [4505.3000, 4521.1700, 4495.3100],\n",
      "        [4458.9700, 4508.6700, 4458.9700],\n",
      "        [4406.6600, 4421.7600, 4393.8200],\n",
      "        [4364.1500, 4418.0300, 4353.3400],\n",
      "        [4391.4100, 4393.4000, 4343.9400],\n",
      "        [4384.3700, 4391.2000, 4359.7600],\n",
      "        [4366.2100, 4386.2600, 4355.4100],\n",
      "        [4364.2700, 4372.2100, 4347.5300],\n",
      "        [4334.2300, 4373.6200, 4334.2300],\n",
      "        [4268.2600, 4319.7200, 4268.2600],\n",
      "        [4201.2700, 4245.6400, 4197.7400],\n",
      "        [4171.3300, 4195.5500, 4153.1200],\n",
      "        [4139.3900, 4177.4700, 4132.9400],\n",
      "        [4152.9300, 4156.7000, 4103.7800],\n",
      "        [4175.9900, 4183.6000, 4127.9000],\n",
      "        [4232.4200, 4232.4200, 4181.4200],\n",
      "        [4235.7900, 4259.3800, 4219.4300],\n",
      "        [4210.4000, 4255.8400, 4189.2200],\n",
      "        [4273.8500, 4276.5600, 4223.0300],\n",
      "        [4321.3600, 4339.5400, 4269.6900],\n",
      "        [4357.3500, 4364.2000, 4303.8400],\n",
      "        [4345.2300, 4393.5700, 4337.5400],\n",
      "        [4342.3700, 4383.3300, 4342.3700],\n",
      "        [4360.4900, 4377.1000, 4311.9700],\n",
      "        [4380.9400, 4385.8500, 4325.4300],\n",
      "        [4366.5900, 4378.6400, 4345.3400],\n",
      "        [4339.7500, 4385.4600, 4339.6400],\n",
      "        [4289.0200, 4341.7300, 4283.7900],\n",
      "        [4234.7900, 4324.1000, 4219.5500],\n",
      "        [4259.3100, 4267.1300, 4225.9100],\n",
      "        [4233.8300, 4268.5000, 4220.4800],\n",
      "        [4269.7500, 4281.1500, 4216.4500],\n",
      "        [4284.5200, 4300.5800, 4260.2100],\n",
      "        [4328.1800, 4333.1500, 4274.8600],\n",
      "        [4269.6500, 4317.2700, 4264.3800],\n",
      "        [4282.6300, 4292.0700, 4238.6300],\n",
      "        [4312.8800, 4313.0100, 4265.9800],\n",
      "        [4310.6200, 4338.5100, 4302.7000],\n",
      "        [4341.7400, 4357.4000, 4316.4900],\n",
      "        [4374.3600, 4375.7000, 4329.1700],\n",
      "        [4452.8100, 4461.0300, 4401.3800],\n",
      "        [4445.4100, 4449.8500, 4416.6100],\n",
      "        [4445.1300, 4466.3600, 4442.1100],\n",
      "        [4497.9800, 4497.9800, 4447.2100],\n",
      "        [4487.7800, 4511.9900, 4478.6900],\n",
      "        [4462.6500, 4479.3900, 4453.5200],\n",
      "        [4473.2700, 4487.1100, 4456.8300],\n",
      "        [4480.9800, 4490.7700, 4467.8900],\n",
      "        [4451.3000, 4473.5300, 4448.3800],\n",
      "        [4434.5500, 4457.8100, 4430.4600],\n",
      "        [4490.3500, 4490.3500, 4442.3800],\n",
      "        [4510.0600, 4514.2900, 4496.0100],\n",
      "        [4530.8500, 4541.2500, 4501.3500],\n",
      "        [4517.0100, 4532.2600, 4507.3900],\n",
      "        [4500.3400, 4521.6500, 4493.5900],\n",
      "        [4432.7500, 4500.1400, 4431.6800],\n",
      "        [4426.0300, 4439.5600, 4414.9800],\n",
      "        [4389.3800, 4418.4600, 4356.2900],\n",
      "        [4455.1600, 4458.3000, 4375.5500],\n",
      "        [4396.4400, 4443.1800, 4396.4400],\n",
      "        [4415.3300, 4418.5900, 4382.7700],\n",
      "        [4380.2800, 4407.5500, 4360.3000],\n",
      "        [4344.8800, 4381.8200, 4335.3100],\n",
      "        [4416.3200, 4421.1700, 4364.8300],\n",
      "        [4433.7900, 4449.9500, 4403.5500],\n",
      "        [4478.8700, 4478.8700, 4432.1900],\n",
      "        [4458.1300, 4490.3300, 4453.4400],\n",
      "        [4450.6900, 4476.2300, 4443.9800],\n",
      "        [4487.1600, 4527.3700, 4457.9200],\n",
      "        [4501.5700, 4502.4400, 4461.3300],\n",
      "        [4498.0300, 4503.3100, 4464.3900],\n",
      "        [4491.5800, 4519.8400, 4491.1500],\n",
      "        [4513.9600, 4540.3400, 4474.5500],\n",
      "        [4494.2700, 4519.4900, 4485.5400],\n",
      "        [4550.9300, 4550.9300, 4505.7500],\n",
      "        [4578.8300, 4584.6200, 4567.5300],\n",
      "        [4584.8200, 4594.2200, 4573.1400],\n",
      "        [4565.7500, 4590.1600, 4564.0100],\n",
      "        [4598.2600, 4607.0700, 4528.5600],\n",
      "        [4558.9600, 4582.4700, 4547.5800],\n",
      "        [4555.1900, 4580.6200, 4552.4200],\n",
      "        [4543.3900, 4563.4100, 4541.2900],\n",
      "        [4550.1600, 4555.0000, 4535.7900],\n",
      "        [4554.3800, 4564.7400, 4527.5600],\n",
      "        [4563.8700, 4578.4300, 4557.4800],\n",
      "        [4521.7800, 4562.3000, 4514.5900],\n",
      "        [4508.8600, 4532.8500, 4504.9000],\n",
      "        [4514.6100, 4527.7600, 4499.5600],\n",
      "        [4491.5000, 4517.3800, 4489.3600],\n",
      "        [4467.6900, 4488.3400, 4463.2300],\n",
      "        [4415.5500, 4443.6400, 4408.4600],\n",
      "        [4394.2300, 4412.6000, 4389.9200],\n",
      "        [4404.5400, 4440.3900, 4397.4000],\n",
      "        [4422.6200, 4422.6200, 4385.0500]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#we want to remove the decimal places that get added there incoorectly\n",
    "features = torch.round(torch.tensor(features, dtype = torch.float64)*100)/100\n",
    "\n",
    "#this is beacuse we want a 1D array for our answerers\n",
    "target = torch.round(torch.tensor(target, dtype = torch.float64).view(-1,1)*100)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class stockPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(stockPredictor, self).__init__()\n",
    "        #this data has 4 inputs than we want to go to 64 i dont not understand why we chose these numbers\n",
    "        self.fc1 = nn.Linear(3,16)\n",
    "        self.fc2 = nn.Linear(16,63)\n",
    "        self.fc3 = nn.Linear(63,32)\n",
    "        self.fc4 = nn.Linear(32,1)\n",
    "        self.double()\n",
    "    \n",
    "    #goes through each layer of the neural netwrok and returns an answer depending\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = stockPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4217.0400],\n",
      "        [4224.1600],\n",
      "        [4278.0000],\n",
      "        [4314.6000],\n",
      "        [4373.2000],\n",
      "        [4373.6300],\n",
      "        [4327.7800],\n",
      "        [4349.6100],\n",
      "        [4376.9500],\n",
      "        [4358.2400],\n",
      "        [4335.6600],\n",
      "        [4308.5000],\n",
      "        [4258.1900],\n",
      "        [4263.7500],\n",
      "        [4229.4500],\n",
      "        [4288.3900],\n",
      "        [4288.0500],\n",
      "        [4299.7000],\n",
      "        [4274.5100],\n",
      "        [4273.5300],\n",
      "        [4337.4400],\n",
      "        [4320.0600],\n",
      "        [4330.0000],\n",
      "        [4402.2000],\n",
      "        [4443.9500],\n",
      "        [4453.5300],\n",
      "        [4450.3200],\n",
      "        [4505.1000],\n",
      "        [4467.4400],\n",
      "        [4461.9000],\n",
      "        [4487.4600],\n",
      "        [4457.4900],\n",
      "        [4451.1400],\n",
      "        [4465.4800],\n",
      "        [4496.8300],\n",
      "        [4515.7700],\n",
      "        [4507.6600],\n",
      "        [4514.8700],\n",
      "        [4497.6300],\n",
      "        [4433.3100],\n",
      "        [4405.7100],\n",
      "        [4376.3100],\n",
      "        [4436.0100],\n",
      "        [4387.5500],\n",
      "        [4399.7700],\n",
      "        [4369.7100],\n",
      "        [4370.3600],\n",
      "        [4404.3300],\n",
      "        [4437.8600],\n",
      "        [4489.7200],\n",
      "        [4464.0500],\n",
      "        [4468.8300],\n",
      "        [4467.7100],\n",
      "        [4499.3800],\n",
      "        [4518.4400],\n",
      "        [4478.0300],\n",
      "        [4501.8900],\n",
      "        [4513.3900],\n",
      "        [4576.7300],\n",
      "        [4588.9600],\n",
      "        [4582.2300],\n",
      "        [4537.4100],\n",
      "        [4566.7500],\n",
      "        [4567.4600],\n",
      "        [4554.6400],\n",
      "        [4536.3400],\n",
      "        [4534.8700],\n",
      "        [4565.7200],\n",
      "        [4554.9800],\n",
      "        [4522.7900],\n",
      "        [4505.4200],\n",
      "        [4510.0400],\n",
      "        [4472.1600],\n",
      "        [4439.2600],\n",
      "        [4409.5300],\n",
      "        [4398.9500],\n",
      "        [4411.5900]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#we need to have the trainning and test in order due to the time relation of the data\n",
    "x_train, x_test = features[0:int(len(features)*0.67),:], features[int(len(features)*0.67):len(features),:]\n",
    "y_train, y_test = target[0:int(len(target)*0.67)], target[int(len(target)*0.67):len(target)]\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500], Loss: 4433184.434816257\n",
      "Epoch [20/500], Loss: 705121.850834409\n",
      "Epoch [30/500], Loss: 428.4625111854679\n",
      "Epoch [40/500], Loss: 37236.9358622485\n",
      "Epoch [50/500], Loss: 55350.38470273139\n",
      "Epoch [60/500], Loss: 10167.545554068507\n",
      "Epoch [70/500], Loss: 1184.5324342595657\n",
      "Epoch [80/500], Loss: 3123.832074842822\n",
      "Epoch [90/500], Loss: 371.55372511392915\n",
      "Epoch [100/500], Loss: 685.3670276714427\n",
      "Epoch [110/500], Loss: 372.17395404863805\n",
      "Epoch [120/500], Loss: 415.9698906736182\n",
      "Epoch [130/500], Loss: 384.97125243232733\n",
      "Epoch [140/500], Loss: 370.75050407523213\n",
      "Epoch [150/500], Loss: 370.99271961958664\n",
      "Epoch [160/500], Loss: 370.62306322932193\n",
      "Epoch [170/500], Loss: 370.8126741684905\n",
      "Epoch [180/500], Loss: 371.0786574344307\n",
      "Epoch [190/500], Loss: 371.0890192269184\n",
      "Epoch [200/500], Loss: 370.97980272112625\n",
      "Epoch [210/500], Loss: 370.86417266046635\n",
      "Epoch [220/500], Loss: 370.7705041196323\n",
      "Epoch [230/500], Loss: 370.6954060968022\n",
      "Epoch [240/500], Loss: 370.63114846266893\n",
      "Epoch [250/500], Loss: 370.57229812799915\n",
      "Epoch [260/500], Loss: 370.5158775060524\n",
      "Epoch [270/500], Loss: 370.460431497961\n",
      "Epoch [280/500], Loss: 370.4052901325042\n",
      "Epoch [290/500], Loss: 370.35015034611774\n",
      "Epoch [300/500], Loss: 370.2948707471874\n",
      "Epoch [310/500], Loss: 370.2393786619637\n",
      "Epoch [320/500], Loss: 370.1836295907319\n",
      "Epoch [330/500], Loss: 370.12758955761643\n",
      "Epoch [340/500], Loss: 370.07122746527745\n",
      "Epoch [350/500], Loss: 370.014512338946\n",
      "Epoch [360/500], Loss: 369.9574134293171\n",
      "Epoch [370/500], Loss: 369.89990199126106\n",
      "Epoch [380/500], Loss: 369.84195354589923\n",
      "Epoch [390/500], Loss: 369.7835494086677\n",
      "Epoch [400/500], Loss: 369.7246766768921\n",
      "Epoch [410/500], Loss: 369.6653267125643\n",
      "Epoch [420/500], Loss: 369.6054929939118\n",
      "Epoch [430/500], Loss: 369.5451694798286\n",
      "Epoch [440/500], Loss: 369.4843501260525\n",
      "Epoch [450/500], Loss: 369.42302931106514\n",
      "Epoch [460/500], Loss: 369.36120239430466\n",
      "Epoch [470/500], Loss: 369.29886582699584\n",
      "Epoch [480/500], Loss: 369.2360168499402\n",
      "Epoch [490/500], Loss: 369.17265318037147\n",
      "Epoch [500/500], Loss: 369.1087729305807\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "epochs = 500\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    y_pred = model.forward(x_train)\n",
    "\n",
    "    outputs = model(features)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    losses.append(loss.detach().numpy())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch +1) %10 ==0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_eval = model.forward(x_test)\n",
    "    loss = criterion(y_eval,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 4222.74 Expected: 4217.04 Difference: -5.699999999999818\n",
      "Predicted: 4260.6 Expected: 4224.16 Difference: -36.44000000000051\n",
      "Predicted: 4313.96 Expected: 4278.0 Difference: -35.960000000000036\n",
      "Predicted: 4344.94 Expected: 4314.6 Difference: -30.339999999999236\n",
      "Predicted: 4362.7 Expected: 4373.2 Difference: 10.5\n",
      "Predicted: 4359.2 Expected: 4373.63 Difference: 14.430000000000291\n",
      "Predicted: 4353.4 Expected: 4327.78 Difference: -25.61999999999989\n",
      "Predicted: 4367.18 Expected: 4349.61 Difference: -17.57000000000062\n",
      "Predicted: 4365.76 Expected: 4376.95 Difference: 11.1899999999996\n",
      "Predicted: 4358.42 Expected: 4358.24 Difference: -0.18000000000029104\n",
      "Predicted: 4308.94 Expected: 4335.66 Difference: 26.720000000000255\n",
      "Predicted: 4266.17 Expected: 4308.5 Difference: 42.32999999999993\n",
      "Predicted: 4253.22 Expected: 4258.19 Difference: 4.969999999999345\n",
      "Predicted: 4244.24 Expected: 4263.75 Difference: 19.51000000000022\n",
      "Predicted: 4259.19 Expected: 4229.45 Difference: -29.73999999999978\n",
      "Predicted: 4284.36 Expected: 4288.39 Difference: 4.030000000000655\n",
      "Predicted: 4315.08 Expected: 4288.05 Difference: -27.029999999999745\n",
      "Predicted: 4287.55 Expected: 4299.7 Difference: 12.149999999999636\n",
      "Predicted: 4274.05 Expected: 4274.51 Difference: 0.4600000000000364\n",
      "Predicted: 4299.77 Expected: 4273.53 Difference: -26.24000000000069\n",
      "Predicted: 4319.96 Expected: 4337.44 Difference: 17.479999999999563\n",
      "Predicted: 4341.15 Expected: 4320.06 Difference: -21.089999999999236\n",
      "Predicted: 4362.24 Expected: 4330.0 Difference: -32.23999999999978\n",
      "Predicted: 4441.56 Expected: 4402.2 Difference: -39.36000000000058\n",
      "Predicted: 4439.35 Expected: 4443.95 Difference: 4.599999999999454\n",
      "Predicted: 4453.3 Expected: 4453.53 Difference: 0.22999999999956344\n",
      "Predicted: 4483.69 Expected: 4450.32 Difference: -33.36999999999989\n",
      "Predicted: 4495.34 Expected: 4505.1 Difference: 9.760000000000218\n",
      "Predicted: 4467.25 Expected: 4467.44 Difference: 0.18999999999959982\n",
      "Predicted: 4474.57 Expected: 4461.9 Difference: -12.670000000000073\n",
      "Predicted: 4481.67 Expected: 4487.46 Difference: 5.789999999999964\n",
      "Predicted: 4459.9 Expected: 4457.49 Difference: -2.4099999999998545\n",
      "Predicted: 4443.21 Expected: 4451.14 Difference: 7.930000000000291\n",
      "Predicted: 4476.88 Expected: 4465.48 Difference: -11.400000000000546\n",
      "Predicted: 4508.28 Expected: 4496.83 Difference: -11.449999999999818\n",
      "Predicted: 4526.94 Expected: 4515.77 Difference: -11.169999999999163\n",
      "Predicted: 4520.88 Expected: 4507.66 Difference: -13.220000000000255\n",
      "Predicted: 4507.44 Expected: 4514.87 Difference: 7.430000000000291\n",
      "Predicted: 4459.69 Expected: 4497.63 Difference: 37.94000000000051\n",
      "Predicted: 4428.79 Expected: 4433.31 Difference: 4.520000000000437\n",
      "Predicted: 4391.76 Expected: 4405.71 Difference: 13.949999999999818\n",
      "Predicted: 4433.58 Expected: 4376.31 Difference: -57.26999999999953\n",
      "Predicted: 4415.55 Expected: 4436.01 Difference: 20.460000000000036\n",
      "Predicted: 4407.69 Expected: 4387.55 Difference: -20.139999999999418\n",
      "Predicted: 4385.82 Expected: 4399.77 Difference: 13.950000000000728\n",
      "Predicted: 4357.3 Expected: 4369.71 Difference: 12.409999999999854\n",
      "Predicted: 4403.72 Expected: 4370.36 Difference: -33.36000000000058\n",
      "Predicted: 4431.92 Expected: 4404.33 Difference: -27.590000000000146\n",
      "Predicted: 4465.78 Expected: 4437.86 Difference: -27.920000000000073\n",
      "Predicted: 4470.13 Expected: 4489.72 Difference: 19.590000000000146\n",
      "Predicted: 4459.47 Expected: 4464.05 Difference: 4.579999999999927\n",
      "Predicted: 4495.07 Expected: 4468.83 Difference: -26.23999999999978\n",
      "Predicted: 4490.73 Expected: 4467.71 Difference: -23.019999999999527\n",
      "Predicted: 4490.88 Expected: 4499.38 Difference: 8.5\n",
      "Predicted: 4503.29 Expected: 4518.44 Difference: 15.149999999999636\n",
      "Predicted: 4513.42 Expected: 4478.03 Difference: -35.39000000000033\n",
      "Predicted: 4502.33 Expected: 4501.89 Difference: -0.4399999999995998\n",
      "Predicted: 4538.29 Expected: 4513.39 Difference: -24.899999999999636\n",
      "Predicted: 4578.48 Expected: 4576.73 Difference: -1.75\n",
      "Predicted: 4585.78 Expected: 4588.96 Difference: 3.180000000000291\n",
      "Predicted: 4575.56 Expected: 4582.23 Difference: 6.669999999999163\n",
      "Predicted: 4581.85 Expected: 4537.41 Difference: -44.44000000000051\n",
      "Predicted: 4565.57 Expected: 4566.75 Difference: 1.180000000000291\n",
      "Predicted: 4565.1 Expected: 4567.46 Difference: 2.3599999999996726\n",
      "Predicted: 4551.36 Expected: 4554.64 Difference: 3.280000000000655\n",
      "Predicted: 4548.53 Expected: 4536.34 Difference: -12.1899999999996\n",
      "Predicted: 4551.24 Expected: 4534.87 Difference: -16.36999999999989\n",
      "Predicted: 4568.42 Expected: 4565.72 Difference: -2.699999999999818\n",
      "Predicted: 4536.33 Expected: 4554.98 Difference: 18.649999999999636\n",
      "Predicted: 4517.84 Expected: 4522.79 Difference: 4.949999999999818\n",
      "Predicted: 4516.05 Expected: 4505.42 Difference: -10.63000000000011\n",
      "Predicted: 4501.77 Expected: 4510.04 Difference: 8.269999999999527\n",
      "Predicted: 4475.21 Expected: 4472.16 Difference: -3.050000000000182\n",
      "Predicted: 4425.22 Expected: 4439.26 Difference: 14.039999999999964\n",
      "Predicted: 4400.89 Expected: 4409.53 Difference: 8.639999999999418\n",
      "Predicted: 4417.25 Expected: 4398.95 Difference: -18.300000000000182\n",
      "Predicted: 4412.22 Expected: 4411.59 Difference: -0.6300000000001091\n",
      "Correct: 0\n",
      "19.75867387641937\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "resid = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(x_test):\n",
    "        y_val = model.forward(data)\n",
    "        resid.append(round(y_test[i].item(),2) - round(y_val.item(),2))\n",
    "        print(f\"Predicted: {round(y_val.item(),2)} Expected: {round(y_test[i].item(),2)} Difference: {round(y_test[i].item(),2) - round(y_val.item(),2)}\")\n",
    "        if round(y_val.item(),2) == round(y_test[i].item(),2):\n",
    "            num_correct += 1\n",
    "print(f\"Correct: {num_correct}\")\n",
    "print(np.std(resid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 5325.88 Expected: 5354.03 Difference: 28.149999999999636\n",
      "Predicted: 5280.99 Expected: 5291.34 Difference: 10.350000000000364\n",
      "Predicted: 5281.29 Expected: 5283.4 Difference: 2.1099999999996726\n",
      "Predicted: 5243.36 Expected: 5277.51 Difference: 34.150000000000546\n",
      "Predicted: 5249.56 Expected: 5235.48 Difference: -14.080000000000837\n",
      "Predicted: 5276.13 Expected: 5266.95 Difference: -9.180000000000291\n",
      "Predicted: 5306.31 Expected: 5306.04 Difference: -0.27000000000043656\n",
      "Predicted: 5293.19 Expected: 5304.72 Difference: 11.530000000000655\n",
      "Predicted: 5317.03 Expected: 5267.84 Difference: -49.1899999999996\n",
      "Predicted: 5311.73 Expected: 5307.01 Difference: -4.719999999999345\n",
      "Predicted: 5309.29 Expected: 5321.41 Difference: 12.11999999999989\n",
      "Predicted: 5313.09 Expected: 5308.13 Difference: -4.960000000000036\n",
      "Predicted: 5299.0 Expected: 5303.27 Difference: 4.270000000000437\n",
      "Predicted: 5312.79 Expected: 5297.1 Difference: -15.6899999999996\n",
      "Predicted: 5283.11 Expected: 5308.15 Difference: 25.039999999999964\n",
      "Predicted: 5232.45 Expected: 5246.68 Difference: 14.230000000000473\n",
      "Predicted: 5228.99 Expected: 5221.42 Difference: -7.569999999999709\n",
      "Predicted: 5227.14 Expected: 5222.68 Difference: -4.460000000000036\n",
      "Predicted: 5197.57 Expected: 5214.08 Difference: 16.51000000000022\n",
      "Predicted: 5177.84 Expected: 5187.67 Difference: 9.829999999999927\n",
      "Predicted: 5190.63 Expected: 5187.7 Difference: -2.930000000000291\n",
      "Predicted: 5158.36 Expected: 5180.74 Difference: 22.38000000000011\n",
      "Predicted: 5123.58 Expected: 5127.79 Difference: 4.210000000000036\n",
      "Predicted: 5048.16 Expected: 5064.2 Difference: 16.039999999999964\n",
      "Predicted: 5051.59 Expected: 5018.39 Difference: -33.19999999999982\n",
      "Predicted: 5087.07 Expected: 5035.69 Difference: -51.38000000000011\n",
      "Predicted: 5111.02 Expected: 5116.17 Difference: 5.149999999999636\n",
      "Predicted: 5093.79 Expected: 5099.96 Difference: 6.170000000000073\n",
      "Predicted: 5026.88 Expected: 5048.42 Difference: 21.539999999999964\n",
      "Predicted: 5076.23 Expected: 5071.63 Difference: -4.599999999999454\n",
      "Predicted: 5047.94 Expected: 5070.55 Difference: 22.610000000000582\n",
      "Predicted: 5003.06 Expected: 5010.6 Difference: 7.539999999999964\n",
      "Predicted: 4996.19 Expected: 4967.23 Difference: -28.960000000000036\n",
      "Predicted: 5033.4 Expected: 5011.12 Difference: -22.279999999999745\n",
      "Predicted: 5055.01 Expected: 5022.21 Difference: -32.80000000000018\n",
      "Predicted: 5064.01 Expected: 5051.41 Difference: -12.600000000000364\n",
      "Predicted: 5129.09 Expected: 5061.82 Difference: -67.27000000000044\n",
      "Predicted: 5154.86 Expected: 5123.41 Difference: -31.449999999999818\n",
      "Predicted: 5178.89 Expected: 5199.06 Difference: 20.170000000000073\n",
      "Predicted: 5164.15 Expected: 5160.64 Difference: -3.509999999999309\n",
      "Predicted: 5204.22 Expected: 5209.91 Difference: 5.6899999999996\n",
      "Predicted: 5211.2 Expected: 5202.39 Difference: -8.80999999999949\n",
      "Predicted: 5184.09 Expected: 5204.34 Difference: 20.25\n",
      "Predicted: 5220.79 Expected: 5147.21 Difference: -73.57999999999993\n",
      "Predicted: 5208.65 Expected: 5211.49 Difference: 2.8400000000001455\n",
      "Predicted: 5200.64 Expected: 5205.81 Difference: 5.170000000000073\n",
      "Predicted: 5252.57 Expected: 5243.77 Difference: -8.799999999999272\n",
      "Predicted: 5254.74 Expected: 5254.35 Difference: -0.3899999999994179\n",
      "Predicted: 5232.43 Expected: 5248.49 Difference: 16.05999999999949\n",
      "Predicted: 5224.56 Expected: 5203.58 Difference: -20.980000000000473\n",
      "Predicted: 5223.01 Expected: 5218.19 Difference: -4.8200000000006185\n",
      "Predicted: 5240.91 Expected: 5234.18 Difference: -6.729999999999563\n",
      "Predicted: 5253.42 Expected: 5241.53 Difference: -11.890000000000327\n",
      "Predicted: 5196.97 Expected: 5224.62 Difference: 27.649999999999636\n",
      "Predicted: 5153.85 Expected: 5178.51 Difference: 24.659999999999854\n",
      "Predicted: 5160.96 Expected: 5149.42 Difference: -11.539999999999964\n",
      "Predicted: 5123.78 Expected: 5117.09 Difference: -6.6899999999996\n",
      "Predicted: 5161.24 Expected: 5150.48 Difference: -10.760000000000218\n",
      "Predicted: 5170.07 Expected: 5165.31 Difference: -4.759999999999309\n",
      "Predicted: 5147.14 Expected: 5175.27 Difference: 28.13000000000011\n",
      "Predicted: 5111.55 Expected: 5117.94 Difference: 6.389999999999418\n",
      "Predicted: 5161.1 Expected: 5123.69 Difference: -37.410000000000764\n",
      "Predicted: 5144.98 Expected: 5157.36 Difference: 12.38000000000011\n",
      "Predicted: 5111.95 Expected: 5104.76 Difference: -7.1899999999996\n",
      "Predicted: 5096.97 Expected: 5078.65 Difference: -18.32000000000062\n",
      "Predicted: 5137.96 Expected: 5130.95 Difference: -7.010000000000218\n",
      "Predicted: 5114.44 Expected: 5137.08 Difference: 22.640000000000327\n",
      "Predicted: 5086.89 Expected: 5096.27 Difference: 9.38000000000011\n",
      "Predicted: 5069.32 Expected: 5069.76 Difference: 0.4400000000005093\n",
      "Predicted: 5072.62 Expected: 5078.18 Difference: 5.5600000000004\n",
      "Predicted: 5088.45 Expected: 5069.53 Difference: -18.920000000000073\n",
      "Predicted: 5099.9 Expected: 5088.8 Difference: -11.099999999999454\n",
      "Predicted: 5061.45 Expected: 5087.03 Difference: 25.579999999999927\n",
      "Predicted: 4966.68 Expected: 4981.8 Difference: 15.11999999999989\n",
      "Predicted: 4981.64 Expected: 4975.51 Difference: -6.130000000000109\n",
      "Predicted: 5025.5 Expected: 5005.57 Difference: -19.93000000000029\n",
      "Predicted: 5014.43 Expected: 5029.73 Difference: 15.299999999999272\n",
      "Predicted: 4981.54 Expected: 5000.62 Difference: 19.079999999999927\n",
      "Predicted: 4955.92 Expected: 4953.17 Difference: -2.75\n",
      "Predicted: 5033.1 Expected: 5021.84 Difference: -11.260000000000218\n",
      "Predicted: 5013.97 Expected: 5026.61 Difference: 12.639999999999418\n",
      "Predicted: 4995.57 Expected: 4997.91 Difference: 2.3400000000001455\n",
      "Predicted: 4983.17 Expected: 4995.06 Difference: 11.890000000000327\n",
      "Predicted: 4949.37 Expected: 4954.23 Difference: 4.859999999999673\n",
      "Predicted: 4946.36 Expected: 4942.81 Difference: -3.5499999999992724\n",
      "Predicted: 4937.74 Expected: 4958.61 Difference: 20.86999999999989\n",
      "Predicted: 4877.66 Expected: 4906.19 Difference: 28.529999999999745\n",
      "Predicted: 4886.93 Expected: 4845.65 Difference: -41.280000000000655\n",
      "Predicted: 4925.82 Expected: 4924.97 Difference: -0.8499999999994543\n",
      "Predicted: 4906.36 Expected: 4927.93 Difference: 21.57000000000062\n",
      "Predicted: 4894.44 Expected: 4890.97 Difference: -3.469999999999345\n",
      "Predicted: 4886.79 Expected: 4894.16 Difference: 7.369999999999891\n",
      "Predicted: 4888.56 Expected: 4868.55 Difference: -20.01000000000022\n",
      "Predicted: 4857.66 Expected: 4864.6 Difference: 6.940000000000509\n",
      "Predicted: 4857.28 Expected: 4850.43 Difference: -6.849999999999454\n",
      "Predicted: 4811.97 Expected: 4839.81 Difference: 27.840000000000146\n",
      "Predicted: 4765.17 Expected: 4780.94 Difference: 15.769999999999527\n",
      "Predicted: 4734.67 Expected: 4739.21 Difference: 4.539999999999964\n",
      "Predicted: 4769.55 Expected: 4765.98 Difference: -3.5700000000006185\n",
      "Predicted: 4789.76 Expected: 4783.83 Difference: -5.930000000000291\n",
      "Predicted: 4779.83 Expected: 4780.24 Difference: 0.4099999999998545\n",
      "Predicted: 4771.71 Expected: 4783.45 Difference: 11.739999999999782\n",
      "Predicted: 4748.5 Expected: 4756.5 Difference: 8.0\n",
      "Predicted: 4727.24 Expected: 4763.54 Difference: 36.30000000000018\n",
      "Predicted: 4700.97 Expected: 4697.24 Difference: -3.730000000000473\n",
      "Predicted: 4706.78 Expected: 4688.68 Difference: -18.099999999999454\n",
      "Predicted: 4719.95 Expected: 4704.81 Difference: -15.139999999999418\n",
      "Predicted: 4742.86 Expected: 4742.83 Difference: -0.02999999999974534\n",
      "Predicted: 4776.66 Expected: 4769.83 Difference: -6.829999999999927\n",
      "Predicted: 4788.24 Expected: 4783.35 Difference: -4.889999999999418\n",
      "Predicted: 4777.52 Expected: 4781.58 Difference: 4.059999999999491\n",
      "Predicted: 4769.64 Expected: 4774.75 Difference: 5.109999999999673\n",
      "Predicted: 4757.06 Expected: 4754.63 Difference: -2.430000000000291\n",
      "Predicted: 4729.92 Expected: 4746.75 Difference: 16.829999999999927\n",
      "Predicted: 4750.92 Expected: 4698.35 Difference: -52.56999999999971\n",
      "Predicted: 4754.27 Expected: 4768.37 Difference: 14.099999999999454\n",
      "Predicted: 4735.73 Expected: 4740.56 Difference: 4.830000000000837\n",
      "Predicted: 4716.58 Expected: 4719.19 Difference: 2.6099999999996726\n",
      "Predicted: 4720.77 Expected: 4719.55 Difference: -1.2200000000002547\n",
      "Predicted: 4671.05 Expected: 4707.09 Difference: 36.039999999999964\n",
      "Predicted: 4626.09 Expected: 4643.7 Difference: 17.609999999999673\n",
      "Predicted: 4606.04 Expected: 4622.44 Difference: 16.399999999999636\n",
      "Predicted: 4589.29 Expected: 4604.37 Difference: 15.079999999999927\n",
      "Predicted: 4577.18 Expected: 4585.59 Difference: 8.409999999999854\n",
      "Predicted: 4576.98 Expected: 4549.34 Difference: -27.639999999999418\n",
      "Predicted: 4564.71 Expected: 4567.18 Difference: 2.4700000000002547\n",
      "Predicted: 4563.01 Expected: 4569.78 Difference: 6.769999999999527\n",
      "Predicted: 4574.49 Expected: 4594.63 Difference: 20.140000000000327\n",
      "Predicted: 4556.29 Expected: 4567.8 Difference: 11.510000000000218\n",
      "Predicted: 4571.48 Expected: 4550.58 Difference: -20.899999999999636\n",
      "Predicted: 4553.67 Expected: 4554.89 Difference: 1.2200000000002547\n",
      "Predicted: 4555.27 Expected: 4550.43 Difference: -4.8400000000001455\n",
      "Predicted: 4557.41 Expected: 4559.34 Difference: 1.930000000000291\n",
      "Predicted: 4557.45 Expected: 4556.62 Difference: -0.8299999999999272\n",
      "Predicted: 4536.88 Expected: 4538.19 Difference: 1.3099999999994907\n",
      "Predicted: 4529.9 Expected: 4547.38 Difference: 17.480000000000473\n",
      "Predicted: 4511.5 Expected: 4514.02 Difference: 2.5200000000004366\n",
      "Predicted: 4500.92 Expected: 4508.24 Difference: 7.319999999999709\n",
      "Predicted: 4509.3 Expected: 4502.88 Difference: -6.420000000000073\n",
      "Predicted: 4479.25 Expected: 4495.7 Difference: 16.449999999999818\n",
      "Predicted: 4409.51 Expected: 4411.55 Difference: 2.0399999999999636\n",
      "Predicted: 4382.88 Expected: 4415.24 Difference: 32.35999999999967\n",
      "Predicted: 4378.87 Expected: 4347.35 Difference: -31.519999999999527\n",
      "Predicted: 4380.49 Expected: 4382.78 Difference: 2.2899999999999636\n",
      "Predicted: 4371.62 Expected: 4378.38 Difference: 6.760000000000218\n",
      "Predicted: 4363.15 Expected: 4365.98 Difference: 2.8299999999999272\n",
      "Predicted: 4350.44 Expected: 4358.34 Difference: 7.900000000000546\n",
      "Predicted: 4289.23 Expected: 4317.78 Difference: 28.550000000000182\n",
      "Predicted: 4218.4 Expected: 4237.86 Difference: 19.460000000000036\n",
      "Predicted: 4176.18 Expected: 4193.8 Difference: 17.61999999999989\n",
      "Predicted: 4153.17 Expected: 4166.82 Difference: 13.649999999999636\n",
      "Predicted: 4140.58 Expected: 4117.37 Difference: -23.210000000000036\n",
      "Predicted: 4165.47 Expected: 4137.23 Difference: -28.24000000000069\n",
      "Predicted: 4218.05 Expected: 4186.77 Difference: -31.279999999999745\n",
      "Predicted: 4240.94 Expected: 4247.68 Difference: 6.740000000000691\n",
      "19.75867387641937\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "resids = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(x_train):\n",
    "        y_val = model.forward(data)\n",
    "        print(f\"Predicted: {round(y_val.item(),2)} Expected: {round(y_train[i].item(),2)} Difference: {round(y_train[i].item(),2) - round(y_val.item(),2)}\")\n",
    "        resids.append(round(y_train[i].item(),2) - round(y_val.item(),2))\n",
    "        if round(y_val.item(),2) == round(y_train[i].item(),2):\n",
    "            correct += 1\n",
    "print(np.std(resid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
